# -*- coding: utf-8 -*-
"""1ºexerciciolivro.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WTvJAFfD1-eaS8bKxE1jltD4xTGlRePN
"""

import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('/content/housing.csv')

df.head()

df.info()

"""## **Dividindo treino e teste aleatórios**"""

# #Criando um conjunto teste
# import numpy as np
# #definindo A função
# def split_train_test(data, test_ratio):
# #fazendo os dados ficarem aleatórios
#   suffled_indices = np.random.permutation(len(data))
# #Calcula o tamanho % do teste em relação ao conjunto total
#   test_set_size = int(len(data)*test_ratio)
# #pega os primeiros valores para serem teste
#   test_indices = suffled_indices[:test_set_size]
# #pega o restante pare treino
#   train_indices = suffled_indices[test_set_size:]
#   return data.iloc[train_indices], data.iloc[test_indices]

# train_set, test_set =split_train_test(df, 0.2)
# print(len(train_set), "train +", len(test_set), "test")

"""## **Dividindo em teste e treino por meio da biblioteca hashlib (Considerando o último byte do hash)**"""

# import hashlib
# def test_set_check(identifier, test_ratio, hash):
#   return hash(np.int64(identifier)).digest()[-1]<256*test_ratio
# def split_train_test_by_id(data, test_ratio, id_column, hash=hashlib.md5):
#   ids = data[id_column]
#   in_test_set = ids.apply(lambda id_:test_set_check(id_,test_ratio,hash))
#   return data.loc[~in_test_set], data.loc[in_test_set]

# df_with_id= df.reset_index()
# train_set, test_set = split_train_test_by_id(df_with_id, 0.2,"index")

# print(len(train_set), "train +", len(test_set), "test")

"""## **Dividindo em treino e teste com o Scikit-Learn**"""

from sklearn.model_selection import train_test_split
train_set, test_set = train_test_split(df, test_size=0.2, random_state=42)

df.info()

print(len(train_set), "train +", len(test_set), "test")

df.info()

import numpy as np
df["income_cat"]= np.ceil((df["median_income"])/1.5)
df["income_cat"].where(df["income_cat"]<5, 5.9, inplace=True)

df["income_cat"].hist(bins=50,figsize=(10,5))

#fazendo uma amostragem estratificada com base na categoria de renda
from sklearn.model_selection import StratifiedShuffleSplit

split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_index, test_index in split.split(df, df["income_cat"]):
  strat_train_set = df.loc[train_index]
  strat_test_set = df.loc[test_index]

#analise das proporções da catgoria de renda no conjunto de testes
strat_test_set["income_cat"].value_counts() / len(strat_test_set)

#removendo os atriubtos income_Cat para os dados voltarem ao estado originial
for set_ in (strat_train_set, strat_test_set):
  set_.drop("income_cat",axis=1,inplace=True)

df = strat_train_set.copy()
print(df)
df.info()

"""# **Visualizando Dados Geográficos**"""

#Criando um diagrama de dispersão para visualizar os dados de todos os bairros
df.plot(kind="scatter", x="longitude", y = "latitude")

df.plot(kind="scatter", x="longitude", y = "latitude", alpha=0.1)

#observando os preços
df.plot(kind="scatter", x="longitude", y = "latitude",alpha=0.4,
        s=df["population"]/100, label="population",figsize=(10,7),
        c="median_house_value",cmap=plt.get_cmap("jet"),colorbar=True,
        )
plt.legend()

#azul = valores baixos e vermelho = alores altos

"""# **Buscado Correlações**"""

#Coeficiente de correlação padrão (r de Pearson)
corr_matrix = df.corr()
corr_matrix["median_house_value"].sort_values(ascending=False)
#O coeficiente varia de -1 a 1, quando está proximo de 1 significa que existe uma forte correlação poistiva

df.info()

from pandas.plotting import scatter_matrix
attributes = ["median_house_value","median_income","total_rooms","housing_median_age"]
scatter_matrix(df[attributes], figsize=(12,8))

df.plot(kind="scatter", x="median_income", y= "median_house_value", alpha=0.1)

df["rooms_per_household"] =reyçkkh df["total_rooms"]/df["households"]
df["bedrooms_per_room"] = df["total_bedrooms"]/df["total_rooms"]
df["population_per_household"] = df["population"]/df["households"]

df.info()

corr_matrix = df.corr()
corr_matrix["median_house_value"].sort_values(ascending=False)

df.info()

"""# **Preparando os dados para o algoritmo**"""

df = strat_train_set.drop("median_house_value", axis=1)
df_labels = strat_train_set["median_house_value"].copy()

df.info()

from sklearn.impute import SimpleImputer

# Create an instance of SimpleImputer with the desired strategy
imputer = SimpleImputer(strategy="median")

df_num = df.drop("ocean_proximity", axis =1)

imputer.fit(df_num)

print(df_num)

imputer.statistics_

df_num.median().values

X = imputer.transform(df_num)
df_tr = pd.DataFrame(X, columns=df_num.columns)

df_cat = df["ocean_proximity"]
df_cat.head(10)

df_cat_encoded, df_categories = df_cat.factorize()
df_cat_encoded[:10]

df_categories

#one-hot encoding
from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder()
df_cat_1hot = encoder.fit_transform(df_cat_encoded.reshape(-1,1))
df_cat_1hot

df_cat_1hot.toarray()

df_categories

"""# **Customize Transformadores**"""

#esse transformador tem um hiperparametro add_bedrooms_per_room, que me mostra se a adição desse atributo ajuda ou não os algoritmos de aprendizado de máquina
from sklearn.base import BaseEstimator, TransformerMixin
rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6
class CombinedAttributesAdder(BaseEstimator, TransformerMixin):
  def __init__(self, add_bedrooms_per_room = True): #sem *args ou *kargs
    self.add_bedrooms_per_room = add_bedrooms_per_room
  def fit(self, X, y=None):
    return self
  def transform(self, X, y=None):
    rooms_per_household = X[:, rooms_ix] / X[:, household_ix]
    population_per_household = X[:, population_ix] / X[:, household_ix]
    if self.add_bedrooms_per_room:
      bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]
      return np.c_[X,rooms_per_household, population_per_household, bedrooms_per_room]
    else:
      return np.c_[X, rooms_per_household, population_per_household]
attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)
df_extra_attribs = attr_adder.transform(df.values)

"""# **Pipelines de transformação**"""

from sklearn.base import BaseEstimator, TransformerMixin

class DataFrameSelector(BaseEstimator, TransformerMixin):
  def __init__(self, attribute_names):
    self.attribute_names = attribute_names
  def fit(self, X, y=None):
    return self
  def transform(self, X):
    return X[self.attribute_names].values

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline  import FeatureUnion

num_attribs = list(df_num)
cat_attribs = ["ocean_proximity"]

num_pipeline = Pipeline([
    ('selector', DataFrameSelector(num_attribs)),
    ('imputer', SimpleImputer(strategy="median")),
    ('attribs_adder', CombinedAttributesAdder()),
    ('std_scaler', StandardScaler()),
])

cat_pipeline = Pipeline([
    ('selector', DataFrameSelector(cat_attribs)),
    ('imputer', SimpleImputer(strategy="most_frequent")),
    ('onehot', OneHotEncoder()),
])

full_pipeline = FeatureUnion(
    transformer_list=[
        ("num_pipeline", num_pipeline),
        ("cat_pipeline", cat_pipeline),
    ])

df_prepared = full_pipeline.fit_transform(df)
df_prepared.shape

"""# **Treinando um modelo de regressão linear**"""

from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(df_prepared, df_labels)

some_data = df.iloc[:5]
some_labels = df_labels.iloc[:5]
some_data_prepared = full_pipeline.transform(some_data)
print("Predictions:", lin_reg.predict(some_data_prepared))
print("Labels:", list(some_labels))

#medindo RMSE (Raíz do erro quadrado médio)

from sklearn.metrics import mean_squared_error
df_predictions = lin_reg.predict(df_prepared)
lin_mse = mean_squared_error(df_labels, df_predictions)
lin_rmse =np.sqrt(lin_mse)
lin_rmse

#Esse valor de erro não é bom pois os valores da casa variam de 120 mil a 265 mil (O modelo não é poderoso o suficiente)

"""# **Decision Tree**"""

from sklearn.tree import DecisionTreeRegressor

tree_reg = DecisionTreeRegressor()
tree_reg.fit(df_prepared, df_labels)

df_preditcions = tree_reg.predict(df_prepared)
tree_mse = mean_squared_error(df_labels, df_preditcions)
tree_rmse = np.sqrt(tree_mse)
tree_rmse

"""# **Utilizando validação cruzada k-fold**"""

#Calculando para a árvore de decisão
from sklearn.model_selection import cross_val_score
scores = cross_val_score(tree_reg, df_prepared, df_labels,
                         scoring="neg_mean_squared_error", cv=10)
tree_rmse_scores = np.sqrt(-scores)

#Calculando para a árvore de decisão
def display_scores(scores):
  print("Scores:", scores)
  print("Mean:", scores.mean())
  print("Standart Deviation:", scores.std())
display_scores(tree_rmse_scores)

lin_scores = cross_val_score(lin_reg, df_prepared, df_labels,
                         scoring="neg_mean_squared_error", cv=10)
lin_rmse_scores = np.sqrt(-lin_scores)
display_scores(lin_rmse_scores)

"""# **Random Forest Regressor**"""

from sklearn.ensemble import RandomForestRegressor
forest_reg = RandomForestRegressor()
forest_reg.fit(df_prepared, df_labels)

# forest_scores = cross_val_score(forest_reg, df_prepared, df_labels,
#                          scoring="neg_mean_squared_error", cv=10)
# forest_rmse_scores = np.sqrt(-forest_scores)
# display_scores(forest_rmse_scores)

"""# **Ajustando o modelo**"""

#Grid Search, achando a melhor combinação de hiperparametros para o randomforestretregressor

from sklearn.model_selection import GridSearchCV

param_grid = [
    {'n_estimators': [3, 10, 30], 'max_features': [2, 4 ,6 , 8]},
    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},
]

forest_reg = RandomForestRegressor()
grid_search = GridSearchCV(forest_reg, param_grid, cv=5,
                           scoring='neg_mean_squared_error')

grid_search.fit(df_prepared, df_labels)

grid_search.best_params_

#Aqui passei os hiperparâmetros ajustados
grid_search.best_estimator_
RandomForestRegressor(
    bootstrap=True, criterion='mse', max_depth=None,
    max_features=8, max_leaf_nodes=None, min_impurity_decrease=0.0,
    min_samples_leaf=1,
    min_samples_split=2,
    n_estimators=30, n_jobs=1, oob_score=False, random_state=42,
    verbose=0, warm_start=False
)

cvres = grid_search.cv_results_
for mean_score, params in zip(cvres["mean_test_score"], cvres["params"]):
    print(np.sqrt(-mean_score), params)

#observando a importância de cada atributo
feature_importances = grid_search.best_estimator_.feature_importances_
feature_importances

extra_attribs = ["rooms_per_hols","pop_per_hold","bedrooms_per_room"]
cat_encoder = cat_pipeline.named_steps["onehot"]
cat_one_hot_attribs = list(cat_encoder.categories[0])
attributes = num_attribs + extra_attribs + cat_one_hot_attribs
sorted(zip(feature_importances, attributes), reverse = True)

